{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1\n",
      "iteration:  2\n",
      "iteration:  3\n",
      "iteration:  4\n",
      "iteration:  5\n",
      "iteration:  6\n",
      "iteration:  7\n",
      "iteration:  8\n",
      "iteration:  9\n",
      "iteration:  10\n",
      "iteration:  11\n",
      "iteration:  12\n",
      "iteration:  13\n",
      "iteration:  14\n",
      "iteration:  15\n",
      "iteration:  16\n",
      "iteration:  17\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "\n",
    "# Created by: Prof. Valdecy Pereira, D.Sc.\n",
    "# UFF - Universidade Federal Fluminense (Brazil)\n",
    "# email:  valdecy.pereira@gmail.com\n",
    "# Course: Data Mining\n",
    "# Lesson: Decision Trees - CART\n",
    "\n",
    "# Citation: \n",
    "# PEREIRA, V. (2018). Project: CART, File: Python-DM-Classification-02-CART.py, GitHub repository: <https://github.com/Valdecy/CART>\n",
    "\n",
    "############################################################################\n",
    "\n",
    "# Installing Required Libraries\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from random import randint\n",
    "from scipy import stats\n",
    "from copy import deepcopy\n",
    "\n",
    "# Function: Returns True, if a Column is Numeric\n",
    "def is_number(string):\n",
    "    for i in range(0, len(string)):\n",
    "        if pd.isnull(string[i]) == False:          \n",
    "            try:\n",
    "                float(string[i])\n",
    "                return True\n",
    "            except ValueError:\n",
    "                return False\n",
    "\n",
    "# Function: Returns True, if a Value is Numeric\n",
    "def is_number_value(value):\n",
    "    if pd.isnull(value) == False:          \n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "# Function: Performs a Chi_Squared Test or Fisher Exact Test           \n",
    "def chi_squared_test(label_df, feature_df):\n",
    "    label_df.reset_index(drop=True, inplace=True)\n",
    "    feature_df.reset_index(drop=True, inplace=True)\n",
    "    data = pd.concat([pd.DataFrame(label_df.values.reshape((label_df.shape[0], 1))), feature_df], axis = 1)\n",
    "    data.columns=[\"label\", \"feature\"]\n",
    "    contigency_table = pd.crosstab(data.iloc[:,0], data.iloc[:,1], margins = False)\n",
    "    m = contigency_table.values.sum()\n",
    "    if m <= 10000 and contigency_table.shape == (2,2):\n",
    "        p_value = stats.fisher_exact(contigency_table)\n",
    "    else:\n",
    "        p_value = stats.chi2_contingency(contigency_table, correction = False) # (No Yates' Correction)\n",
    "    return p_value[1]\n",
    "\n",
    "# Function: Prediction           \n",
    "def prediction_dt_cart(model, Xdata):\n",
    "    Xdata = Xdata.reset_index(drop=True)\n",
    "    ydata = pd.DataFrame(index=range(0, Xdata.shape[0]), columns=[\"Prediction\"])\n",
    "    data  = pd.concat([ydata, Xdata], axis = 1)\n",
    "    rule = []\n",
    "    \n",
    "    # Preprocessing - Boolean\n",
    "    for j in range(0, data.shape[1]):\n",
    "        if data.iloc[:,j].dtype == \"bool\":\n",
    "            data.iloc[:,j] = data.iloc[:, j].astype(str)\n",
    "   \n",
    "    dt_model = deepcopy(model)\n",
    "\n",
    "    count = 0\n",
    "    end_count = data.shape[1]\n",
    "    while (count < end_count-1):\n",
    "        count = count + 1\n",
    "        if is_number(data.iloc[:, 1]) == False:\n",
    "            col_name = data.iloc[:, 1].name\n",
    "            new_col  = data.iloc[:, 1].unique()\n",
    "            for k in range(0, len(new_col)):\n",
    "                one_hot_data = data.iloc[:, 1]\n",
    "                one_hot_data = pd.DataFrame({str(col_name) + \"[\" + str(new_col[k]) + \"]\": data.iloc[:, 1]})\n",
    "                for L in range (0, one_hot_data.shape[0]):\n",
    "                    if one_hot_data.iloc[L, 0] == new_col[k]:\n",
    "                        one_hot_data.iloc[L, 0] = \" 1 \"\n",
    "                    else:\n",
    "                        one_hot_data.iloc[L, 0] = \" 0 \"\n",
    "                data = pd.concat([data, one_hot_data.astype(np.int32)], axis = 1)\n",
    "            data.drop(col_name, axis = 1, inplace = True)\n",
    "            end_count = data.shape[1]\n",
    "        else:\n",
    "            col_name = data.iloc[:, 1].name\n",
    "            one_hot_data = data.iloc[:, 1]\n",
    "            data.drop(col_name, axis = 1, inplace = True)\n",
    "            data = pd.concat([data, one_hot_data], axis = 1)\n",
    "\n",
    "    # Preprocessing - Binary Values\n",
    "    for i in range(0, data.shape[0]):\n",
    "        for j in range(1, data.shape[1]):\n",
    "            if data.iloc[:,j].dropna().value_counts().index.isin([0,1]).all():\n",
    "               if data.iloc[i,j] == 0:\n",
    "                   data.iloc[i,j] = str(0)\n",
    "               else:\n",
    "                   data.iloc[i,j] = str(1)\n",
    "    \n",
    "    for i in range(0, len(dt_model)):\n",
    "        dt_model[i] = dt_model[i].replace(\"{\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"}\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\";\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"IF \", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"AND\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"THEN\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"=\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"<\", \"<=\")\n",
    "        dt_model[i] = dt_model[i].replace(\" 0 \", \"<=0\")\n",
    "        dt_model[i] = dt_model[i].replace(\" 1 \", \">0\")\n",
    "    \n",
    "    for i in range(0, len(dt_model) -2): \n",
    "        splited_rule = [x for x in dt_model[i].split(\" \") if x]\n",
    "        rule.append(splited_rule)\n",
    "   \n",
    "    for i in range(0, Xdata.shape[0]): \n",
    "        for j in range(0, len(rule)):\n",
    "            rule_confirmation = len(rule[j])/2 - 1\n",
    "            rule_count = 0\n",
    "            for k in range(0, len(rule[j]) - 2, 2):\n",
    "                if (rule[j][k] in list(data.columns.values)) == False:\n",
    "                    zeros = pd.DataFrame(0, index = range(0, data.shape[0]), columns = [rule[j][k]])\n",
    "                    data  = pd.concat([data, zeros], axis = 1)\n",
    "                if is_number_value(data[rule[j][k]][i]) == False:\n",
    "                    if (data[rule[j][k]][i] in rule[j]):\n",
    "                        rule_count = rule_count + 1\n",
    "                        if (rule_count == rule_confirmation):\n",
    "                            data.iloc[i,0] = rule[j][len(rule[j]) - 1]\n",
    "                    else:\n",
    "                        k = len(rule[j])\n",
    "                elif is_number_value(data[rule[j][k]][i]) == True:\n",
    "                     if rule[j][k+1].find(\"<=\") == 0:\n",
    "                         if float(data[rule[j][k]][i]) <= float(rule[j][k+1].replace(\"<=\", \"\")): \n",
    "                             rule_count = rule_count + 1\n",
    "                             if (rule_count == rule_confirmation):\n",
    "                                 data.iloc[i,0] = rule[j][len(rule[j]) - 1]\n",
    "                         else:\n",
    "                             k = len(rule[j])\n",
    "                     elif rule[j][k+1].find(\">\") == 0:\n",
    "                         if float(data[rule[j][k]][i]) > float(rule[j][k+1].replace(\">\", \"\")): \n",
    "                             rule_count = rule_count + 1\n",
    "                             if (rule_count == rule_confirmation):\n",
    "                                 data.iloc[i,0] = rule[j][len(rule[j]) - 1]\n",
    "                         else:\n",
    "                             k = len(rule[j])\n",
    "    \n",
    "    for i in range(0, Xdata.shape[0]):\n",
    "        if pd.isnull(data.iloc[i,0]):\n",
    "            data.iloc[i,0] = dt_model[len(dt_model)-1]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function: Calculates the Gini Index  \n",
    "def gini_index(target, feature = [], uniques = []):\n",
    "    gini = 0\n",
    "    weighted_gini = 0\n",
    "    denominator_1 = feature.count()\n",
    "    data = pd.concat([pd.DataFrame(target.values.reshape((target.shape[0], 1))), feature], axis = 1)\n",
    "    for word in range(0, len(uniques)):\n",
    "        denominator_2 = feature[(feature == uniques[word])].count() #12\n",
    "        if denominator_2[0] > 0:\n",
    "            for lbl in range(0, len(np.unique(target))):\n",
    "                numerator_1 = data.iloc[:,0][(data.iloc[:,0] == np.unique(target)[lbl]) & (data.iloc[:,1]  == uniques[word])].count()\n",
    "                if numerator_1 > 0:\n",
    "                    gini = gini + (numerator_1/denominator_2)**2 \n",
    "        gini = 1 - gini\n",
    "        weighted_gini = weighted_gini + gini*(denominator_2/denominator_1)\n",
    "        gini = 0\n",
    "    return float(weighted_gini)\n",
    "\n",
    "# Function: Binary Split on Continuous Variables \n",
    "def split_me(feature, split):\n",
    "    result = pd.DataFrame(feature.values.reshape((feature.shape[0], 1)))\n",
    "    for fill in range(0, len(feature)):\n",
    "        result.iloc[fill,0] = feature.iloc[fill]\n",
    "    lower = \"<=\" + str(split)\n",
    "    upper = \">\" + str(split)\n",
    "    for convert in range(0, len(feature)):\n",
    "        if float(feature.iloc[convert]) <= float(split):\n",
    "            result.iloc[convert,0] = lower\n",
    "        else:\n",
    "            result.iloc[convert,0] = upper\n",
    "    binary_split = []\n",
    "    binary_split = [lower, upper]\n",
    "    return result, binary_split\n",
    "\n",
    "# Function: CART Algorithm\n",
    "def dt_cart(Xdata, ydata, cat_missing = \"none\", num_missing = \"none\", pre_pruning = \"none\", chi_lim = 0.1, min_lim = 5):\n",
    "    \n",
    "    ################     Part 1 - Preprocessing    #############################\n",
    "    # Preprocessing - Creating Dataframe\n",
    "    name = ydata.name\n",
    "    ydata = pd.DataFrame(ydata.values.reshape((ydata.shape[0], 1)))\n",
    "    for j in range(0, ydata.shape[1]):\n",
    "        if ydata.iloc[:,j].dropna().value_counts().index.isin([0,1]).all():\n",
    "            for i in range(0, ydata.shape[0]):          \n",
    "               if ydata.iloc[i,j] == 0:\n",
    "                   ydata.iloc[i,j] = \"zero\"\n",
    "               else:\n",
    "                   ydata.iloc[i,j] = \"one\"\n",
    "    dataset = pd.concat([ydata, Xdata], axis = 1)\n",
    "    \n",
    "     # Preprocessing - Boolean Values\n",
    "    for j in range(0, dataset.shape[1]):\n",
    "        if dataset.iloc[:,j].dtype == \"bool\":\n",
    "            dataset.iloc[:,j] = dataset.iloc[:, j].astype(str)\n",
    "\n",
    "    # Preprocessing - Missing Values\n",
    "    if cat_missing != \"none\":\n",
    "        for j in range(1, dataset.shape[1]): \n",
    "            if is_number(dataset.iloc[:, j]) == False:\n",
    "                for i in range(0, dataset.shape[0]):\n",
    "                    if pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                        if cat_missing == \"missing\":\n",
    "                            dataset.iloc[i,j] = \"Unknow\"\n",
    "                        elif cat_missing == \"most\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].value_counts().idxmax()\n",
    "                        elif cat_missing == \"remove\":\n",
    "                            dataset = dataset.drop(dataset.index[i], axis = 0)\n",
    "                        elif cat_missing == \"probability\":\n",
    "                            while pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                                dataset.iloc[i,j] = dataset.iloc[randint(0, dataset.shape[0] - 1), j]            \n",
    "    elif num_missing != \"none\":\n",
    "            if is_number(dataset.iloc[:, j]) == True:\n",
    "                for i in range(0, dataset.shape[0]):\n",
    "                    if pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                        if num_missing == \"mean\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].mean()\n",
    "                        elif num_missing == \"median\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].median()\n",
    "                        elif num_missing == \"most\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].value_counts().idxmax()\n",
    "                        elif cat_missing == \"remove\":\n",
    "                            dataset = dataset.drop(dataset.index[i], axis = 0)\n",
    "                        elif num_missing == \"probability\":\n",
    "                            while pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                                dataset.iloc[i,j] = dataset.iloc[randint(0, dataset.shape[0] - 1), j]  \n",
    "   \n",
    "    # Preprocessing - One Hot Encode\n",
    "    count = 0\n",
    "    end_count = dataset.shape[1]\n",
    "    while (count < end_count-1):\n",
    "        count = count + 1\n",
    "        if is_number(dataset.iloc[:, 1]) == False:\n",
    "            col_name = dataset.iloc[:, 1].name\n",
    "            new_col  = dataset.iloc[:, 1].unique()\n",
    "            for k in range(0, len(new_col)):\n",
    "                one_hot_data = dataset.iloc[:, 1]\n",
    "                one_hot_data = pd.DataFrame({str(col_name) + \"[\" + str(new_col[k]) + \"]\": dataset.iloc[:, 1]})\n",
    "                for L in range (0, one_hot_data.shape[0]):\n",
    "                    if one_hot_data.iloc[L, 0] == new_col[k]:\n",
    "                        one_hot_data.iloc[L, 0] = \" 1 \"\n",
    "                    else: \n",
    "                        one_hot_data.iloc[L, 0] = \" 0 \"\n",
    "                dataset = pd.concat([dataset, one_hot_data.astype(np.int32)], axis = 1)\n",
    "            dataset.drop(col_name, axis = 1, inplace = True)\n",
    "            end_count = dataset.shape[1]\n",
    "        else:\n",
    "            col_name = dataset.iloc[:, 1].name\n",
    "            one_hot_data = dataset.iloc[:, 1]\n",
    "            dataset.drop(col_name, axis = 1, inplace = True)\n",
    "            dataset = pd.concat([dataset, one_hot_data], axis = 1)\n",
    "    \n",
    "    bin_names = list(dataset)\n",
    "     \n",
    "    # Preprocessing - Binary Values\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        for j in range(1, dataset.shape[1]):\n",
    "            if dataset.iloc[:,j].dropna().value_counts().index.isin([0,1]).all():\n",
    "               bin_names[j] = \"binary\"\n",
    "               if dataset.iloc[i,j] == 0:\n",
    "                   dataset.iloc[i,j] = str(0)\n",
    "               else:\n",
    "                   dataset.iloc[i,j] = str(1)\n",
    "                \n",
    "    # Preprocessing - Unique Words List\n",
    "    unique = []\n",
    "    uniqueWords = []\n",
    "    for j in range(0, dataset.shape[1]): \n",
    "        for i in range(0, dataset.shape[0]):\n",
    "            token = dataset.iloc[i, j]\n",
    "            if not token in unique:\n",
    "                unique.append(token)\n",
    "        uniqueWords.append(unique)\n",
    "        unique = []  \n",
    "    \n",
    "    # Preprocessing - Label Matrix\n",
    "    label = np.array(uniqueWords[0])\n",
    "    label = label.reshape(1, len(uniqueWords[0]))\n",
    "    \n",
    "    ################    Part 2 - Initialization    #############################\n",
    "    # CART - Initializing Variables\n",
    "    i = 0\n",
    "    branch = [None]*1\n",
    "    branch[0] = dataset\n",
    "    gini_vector = np.empty([1, branch[i].shape[1]])\n",
    "    lower = \" 0 \"\n",
    "    root_index = 0\n",
    "    rule = [None]*1\n",
    "    rule[0] = \"IF \"\n",
    "    skip_update = False\n",
    "    stop = 2\n",
    "    upper = \" 1 \"\n",
    "    \n",
    "    ################     Part 3 - CART Algorithm    #############################\n",
    "    # CART - Algorithm\n",
    "    while (i < stop):\n",
    "        gini_vector.fill(1)\n",
    "        for element in range(1, branch[i].shape[1]):\n",
    "            if len(branch[i]) == 0:\n",
    "                skip_update = True \n",
    "                break\n",
    "            if len(np.unique(branch[i][0])) == 1 or len(branch[i]) == 1:\n",
    "                 if \";\" not in rule[i]:\n",
    "                     rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].iloc[0, 0] + \"\"\n",
    "                     rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                     if i == 1 and (rule[i].find(\"{0}\") != -1 or rule[i].find(\"{1}\")!= -1):\n",
    "                         rule[i] = rule[i].replace(\";\", \"\")\n",
    "                 skip_update = True\n",
    "                 break\n",
    "            if i > 0 and is_number(dataset.iloc[:, element]) == False and pre_pruning == \"chi_2\" and chi_squared_test(branch[i].iloc[:, 0], branch[i].iloc[:, element]) > chi_lim:\n",
    "                 if \";\" not in rule[i]:\n",
    "                     rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \";\"\n",
    "                     rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                 skip_update = True\n",
    "                 continue\n",
    "            if is_number(dataset.iloc[:, element]) == True and bin_names[element] != \"binary\":\n",
    "                gini_vector[0, element] = 1.0\n",
    "                value = branch[i].iloc[:, element].unique()\n",
    "                skip_update = False\n",
    "                for bin_split in range(0, len(value)):\n",
    "                    bin_sample = split_me(feature = branch[i].iloc[:, element], split = value[bin_split])\n",
    "                    if i > 0 and pre_pruning == \"chi_2\" and chi_squared_test(branch[i].iloc[:, 0], bin_sample[0]) > chi_lim:\n",
    "                        if \";\" not in rule[i]:\n",
    "                             rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \";\"\n",
    "                             rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                        skip_update = True\n",
    "                        continue\n",
    "                    g_index = gini_index(target = branch[i].iloc[:, 0], feature = bin_sample[0], uniques = bin_sample[1])\n",
    "                    if g_index < float(gini_vector[0, element]):\n",
    "                        gini_vector[0, element] = g_index\n",
    "                        uniqueWords[element] = bin_sample[1]\n",
    "            if (is_number(dataset.iloc[:, element]) == False or bin_names[element] == \"binary\"):\n",
    "                gini_vector[0, element] = 1.0\n",
    "                skip_update = False\n",
    "                g_index = gini_index(target = branch[i].iloc[:, 0], feature =  pd.DataFrame(branch[i].iloc[:, element].values.reshape((branch[i].iloc[:, element].shape[0], 1))), uniques = uniqueWords[element])\n",
    "                gini_vector[0, element] = g_index\n",
    "            if i > 0 and pre_pruning == \"min\" and len(branch[i]) <= min_lim:\n",
    "                 if \";\" not in rule[i]:\n",
    "                     rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \";\"\n",
    "                     rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                 skip_update = True\n",
    "                 continue\n",
    "                   \n",
    "        if skip_update == False:\n",
    "            root_index = np.argmin(gini_vector)\n",
    "            rule[i] = rule[i] + list(branch[i])[root_index]          \n",
    "            for word in range(0, len(uniqueWords[root_index])):\n",
    "                uw = str(uniqueWords[root_index][word]).replace(\"<=\", \"\")\n",
    "                uw = uw.replace(\">\", \"\")\n",
    "                lower = \"<=\" + uw\n",
    "                upper = \">\" + uw\n",
    "                if uniqueWords[root_index][word] == lower and bin_names[root_index] != \"binary\":\n",
    "                    branch.append(branch[i][branch[i].iloc[:, root_index] <= float(uw)])\n",
    "                elif uniqueWords[root_index][word] == upper and bin_names[root_index] != \"binary\":\n",
    "                    branch.append(branch[i][branch[i].iloc[:, root_index]  > float(uw)])\n",
    "                else:\n",
    "                    branch.append(branch[i][branch[i].iloc[:, root_index] == uniqueWords[root_index][word]])\n",
    "                node = uniqueWords[root_index][word]\n",
    "                rule.append(rule[i] + \" = \" + \"{\" +  str(node) + \"}\")            \n",
    "            for logic_connection in range(1, len(rule)):\n",
    "                if len(np.unique(branch[i][0])) != 1 and rule[logic_connection].endswith(\" AND \") == False  and rule[logic_connection].endswith(\"}\") == True:\n",
    "                    rule[logic_connection] = rule[logic_connection] + \" AND \"\n",
    "        \n",
    "        skip_update = False\n",
    "        i = i + 1\n",
    "        print(\"iteration: \", i)\n",
    "        stop = len(rule)\n",
    "    \n",
    "    for i in range(len(rule) - 1, -1, -1):\n",
    "        if rule[i].endswith(\";\") == False:\n",
    "            del rule[i]    \n",
    "    \n",
    "    rule.append(\"Total Number of Rules: \" + str(len(rule)))\n",
    "    rule.append(dataset.agg(lambda x:x.value_counts().index[0])[0])\n",
    "    \n",
    "    return rule\n",
    "\n",
    "    ############### End of Function ##############\n",
    "\n",
    "######################## Part 4 - Usage ####################################\n",
    "\n",
    "df = pd.read_csv('H:\\RC\\DeepLearning\\DecisionTree\\data\\letter-recognition.data', sep = ',')\n",
    "\n",
    "X = df.iloc[0:9, 1:]\n",
    "Y = df.iloc[0:9, 0]\n",
    "\n",
    "dt_model = dt_cart(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x-box  y-box  width  high  onpix  x-bar  y-bar  x2bar  y2bar  xybar  x2ybr  \\\n",
      "0      2      8      3     5      1      8     13      0      6      6     10   \n",
      "1      5     12      3     7      2     10      5      5      4     13      3   \n",
      "2      4     11      6     8      6     10      6      2      6     10      3   \n",
      "3      7     11      6     6      3      5      9      4      6      4      4   \n",
      "4      2      1      3     1      1      8      6      6      6      6      5   \n",
      "5      4     11      5     8      3      8      8      6      9      5      6   \n",
      "6      4      2      5     4      4      8      7      6      6      7      6   \n",
      "7      1      1      3     2      1      8      2      2      2      8      2   \n",
      "8      2      2      4     4      2     10      6      2      6     12      4   \n",
      "\n",
      "   xy2br  x-ege  xegvy  y-ege  yegvx  \n",
      "0      8      0      8      0      8  \n",
      "1      9      2      8      4     10  \n",
      "2      7      3      7      3      9  \n",
      "3     10      6     10      2      8  \n",
      "4      9      1      7      5     10  \n",
      "5      6      0      8      9      7  \n",
      "6      6      2      8      7     10  \n",
      "7      8      1      6      2      7  \n",
      "8      8      1      6      1      7  \n",
      "  Prediction  x-box  y-box  width  high  onpix  x-bar  y-bar  x2bar  y2bar  \\\n",
      "0          T      2      8      3     5      1      8     13      0      6   \n",
      "1          T      5     12      3     7      2     10      5      5      4   \n",
      "2          T      4     11      6     8      6     10      6      2      6   \n",
      "3          T      7     11      6     6      3      5      9      4      6   \n",
      "4          T      2      1      3     1      1      8      6      6      6   \n",
      "5          T      4     11      5     8      3      8      8      6      9   \n",
      "6          T      4      2      5     4      4      8      7      6      6   \n",
      "7          T      1      1      3     2      1      8      2      2      2   \n",
      "8          T      2      2      4     4      2     10      6      2      6   \n",
      "\n",
      "   xybar  x2ybr  xy2br  x-ege  xegvy  y-ege  yegvx  \n",
      "0      6     10      8      0      8      0      8  \n",
      "1     13      3      9      2      8      4     10  \n",
      "2     10      3      7      3      7      3      9  \n",
      "3      4      4     10      6     10      2      8  \n",
      "4      6      5      9      1      7      5     10  \n",
      "5      5      6      6      0      8      9      7  \n",
      "6      7      6      6      2      8      7     10  \n",
      "7      8      2      8      1      6      2      7  \n",
      "8     12      4      8      1      6      1      7  \n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "test =  df.iloc[0:9, 1:]\n",
    "print(test)\n",
    "result = prediction_dt_cart(dt_model, test)\n",
    "print(result)\n",
    "########################## End of Code #####################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15c1478bb65473417b9ac164de70bca630d8e66abbaabbebf97c1a6f6624a81d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
